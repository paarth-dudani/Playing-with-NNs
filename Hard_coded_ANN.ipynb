{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data \n",
    "\n",
    "\n",
    "# Dataset: {x,y}, 1D (dimesnion=1) x values with their y-values. N = 1000\n",
    "\n",
    "# Layers of NN 2\n",
    "\n",
    "N =20\n",
    "mu_x = 5\n",
    "sigma_x = 2\n",
    "\n",
    "X = np.sort(np.random.normal(mu_x, sigma_x,N))\n",
    "m_true = 2 \n",
    "c_true = 3\n",
    "\n",
    "y_with_noise = np.zeros((N),)\n",
    "\n",
    "mu = 0\n",
    "sigma = 2\n",
    "\n",
    "for i in range(N):\n",
    "    y_with_noise[i] = m_true*X[i] + c_true + np.random.normal(mu, sigma)\n",
    "\n",
    "plt.plot(X,m_true*X+c_true)\n",
    "plt.scatter(X,y_with_noise, s = 3, color = 'red')\n",
    "\n",
    "\n",
    "y_with_noise = y_with_noise[:, np.newaxis]\n",
    "# y` = Ax is the system. \n",
    "\n",
    "Matrix_A = np.vstack([X,np.ones(N)]).T\n",
    "\n",
    "LSq_sol = np.dot(np.matmul(np.linalg.inv(np.matmul(Matrix_A.T,Matrix_A)),Matrix_A.T),y_with_noise)\n",
    "\n",
    "m_lsq, c_lsq = LSq_sol\n",
    "\n",
    "plt.plot(X,m_lsq*X+c_lsq, linestyle= 'dashed', color='black')\n",
    "\n",
    "LSq_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network training.\n",
    "\n",
    "# Initializing\n",
    "units_per_layer = 5\n",
    "W_1 = np.random.normal(0, 1, size =[units_per_layer])\n",
    "W_2 = np.random.normal(0, 1, size =[units_per_layer])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def Relu(x):\n",
    "    if type(x) == float or type(x)== int:\n",
    "        if x>0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    else:\n",
    "        for i in range(len(x)):\n",
    "            if x[i]>0:\n",
    "                x[i] = x[i]\n",
    "                return x\n",
    "            else:\n",
    "                x[i] = 0\n",
    "                return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def der_Relu(x):\n",
    "    if type(x) == np.float64 or type(x)== int:\n",
    "        if x>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    else:\n",
    "        for i in range(len(x)):\n",
    "            if x[i]>0:\n",
    "                x[i] = 1\n",
    "            else:\n",
    "                x[i] = 0\n",
    "        return x\n",
    "    \n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "# Forward prop\n",
    "\n",
    "# Training\n",
    "\n",
    "DelE_delW1 = np.random.normal(0,1,size=units_per_layer)\n",
    "DelE_delW2 = np.random.normal(0,1,size=units_per_layer)\n",
    "diff_y = np.zeros((N))\n",
    "\n",
    "training_steps = 20\n",
    "Error = np.zeros((training_steps))\n",
    "\n",
    "def mse(x,y):\n",
    "    return (x-y)**2\n",
    "\n",
    "\n",
    "for t  in range(training_steps):\n",
    "    for j in range(units_per_layer):\n",
    "        for n in range(N):\n",
    "            Pred_first_layer = tanh(X[n]*W_1)\n",
    "            pred_second_layer = np.matmul(W_2,Pred_first_layer)\n",
    "            diff_y[n] = abs(pred_second_layer-y_with_noise[n])\n",
    "            DelE_delW2[j] += diff_y[n]*Pred_first_layer[j]\n",
    "            DelE_delW1[j] += X[n]*tanh_prime(W_1[j]*X[n])*diff_y[n]*W_2[j]\n",
    "\n",
    "\n",
    "        W_1[j] = W_1[j] - (10**-7)*DelE_delW1[j]\n",
    "        W_2[j] = W_2[j] - (10**-7)*DelE_delW2[j]\n",
    "        En = 0\n",
    "        for n in range(N):\n",
    "            Pred_first_layer = sigmoid(X[n]*W_1)\n",
    "            Pred_first_layer = Pred_first_layer[:,np.newaxis]\n",
    "            pred_second_layer = np.matmul(W_2,Pred_first_layer)\n",
    "            En += (pred_second_layer-y_with_noise[n])**2/(2*len(y_with_noise))\n",
    "    Error[t] = En/N\n",
    "\n",
    "    \n",
    "Error\n",
    "## Error calculation\n",
    "\n",
    "\n",
    "\n",
    "# def prediction_NN(X,n,W_1,W_2):\n",
    "    # Pred_first_layer = sigmoid(X[n]*W_1)\n",
    "    # return np.matmul(W_2,Pred_first_layer)\n",
    "\n",
    "\n",
    "Pred_first_layer = sigmoid(X[0]*W_1)\n",
    "np.matmul(W_2,Pred_first_layer)\n",
    "\n",
    "Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded implementation of ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class FClayer(Layer):\n",
    "    # input_size = no. of input neurons\n",
    "    # output_size = no. of ouput neurons\n",
    "\n",
    "    # Input is a row vector of dimension n (shape = (n,1)) and so are the biases such that Y = W*X.T + B\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.normal(0,1,size= [input_size,output_size]) \n",
    "        self.biases = np.random.normal(0,1,size=[1,output_size]) \n",
    "\n",
    "    def forward_prop(self,input):\n",
    "        self.input = input\n",
    "        self.output = self.input @ self.weights  + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def back_prop(self, output_error,learning_rate):\n",
    "        dEdW =  self.input.T @ output_error \n",
    "        dEdB = output_error\n",
    "        input_error = output_error @ self.weights.T\n",
    "\n",
    "        # update params\n",
    "        self.weights -= dEdW*learning_rate\n",
    "        self.biases -= dEdB*learning_rate\n",
    "\n",
    "        return input_error\n",
    "    \n",
    "class Activation_layer(Layer):\n",
    "\n",
    "    def __init__(self, activation,activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward_prop(self,input):\n",
    "        self.input = input\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    def back_prop(self,output_error,learning_rate):\n",
    "        input_error = np.multiply(output_error,self.activation_prime(self.input))\n",
    "        return input_error\n",
    "\n",
    "# Activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "## Derivative of the activation function\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "# Loss function \n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))/2\n",
    "\n",
    "# Derivative of the loss function\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None \n",
    "\n",
    "    # Add pre-defined layers to the list of layers in the network. It can add any kind of predefined layer such as FC/activation etc.\n",
    "    \n",
    "    def add_layer(self,layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Defining loss functions\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # Predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimesion first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_prop(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "    \n",
    "    # Training the network\n",
    "\n",
    "    def train(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "        Error = []\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                if np.shape(x_train[j])[0] == 1:\n",
    "                    output = x_train[j]\n",
    "                else: \n",
    "                    output = x_train[j].reshape(1,len(x_train[j]))\n",
    "                # Forward propagagtion \n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_prop(output)\n",
    "\n",
    "                err += self.loss(y_train[j],output)\n",
    "                # Backward propagation:\n",
    "\n",
    "                output_error = self.loss_prime(y_train[j],output)\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    output_error = layer.back_prop(output_error,learning_rate)\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   output_error=%f' % (i+1, epochs, err))\n",
    "\n",
    "\n",
    "layer1 = FClayer(2,3)\n",
    "# layer1.forward_prop(np.array([[1,2]]))\n",
    "layer2 = FClayer(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import typing \n",
    "\n",
    "# training data\n",
    "x_train = np.array([[[0,0,0,0]], [[0,1,0,0]], [[1,0,0,0]], [[1,1,0,0]],[[0,0,1,1]], [[0,1,1,1]], [[1,0,1,1]], [[1,1,1,1]]])\n",
    "y_train = np.array([[[0]], [[0]], [[1]], [[1]],[[1]], [[1]], [[0]], [[0]]])\n",
    "np.shape(y_train[0])\n",
    "\n",
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiailzing the network\n",
    "\n",
    "XOR_net = Network()\n",
    "XOR_net.add_layer(FClayer(4, 4))\n",
    "XOR_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "XOR_net.add_layer(FClayer(4, 1))\n",
    "XOR_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "\n",
    "XOR_net.use(mse, mse_prime)\n",
    "XOR_net.train(x_train, y_train, epochs=2000, learning_rate=0.1)\n",
    "\n",
    "out = XOR_net.predict(x_train)\n",
    "print(y_train.T)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "digits.data = digits.data/16\n",
    "\n",
    "x_train = digits.data[0:len(digits.data)-10]\n",
    "x_test = digits.data[len(digits.data)-10:-1]\n",
    "y_train = digits.target[0:len(digits.target)-10]\n",
    "y_test = digits.target[len(digits.target)-10:-1]\n",
    "\n",
    "Image_net = Network()\n",
    "\n",
    "Image_net.add_layer(FClayer(64, 30))\n",
    "Image_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "Image_net.add_layer(FClayer(30, 20))\n",
    "Image_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "Image_net.add_layer(FClayer(20, 1))\n",
    "\n",
    "Image_net.use(mse, mse_prime)\n",
    "Image_net.train(x_train, y_train, epochs=500, learning_rate=0.00005)\n",
    "\n",
    "out = Image_net.predict(x_test)\n",
    "print('prediction is', out)\n",
    "print()\n",
    "print('Actual output is',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing forward and backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[[0.0,0.0]], [[0.0,1.0]], [[1.0,0.0]], [[1.0,1.0]]])\n",
    "# x_train = np.random.normal(0,1,size=(4,1,2))\n",
    "\n",
    "layer1 = FClayer(2,4)\n",
    "\n",
    "output_l1 =layer1.forward_prop(x_train[0])\n",
    "\n",
    "print(output_l1)\n",
    "print()\n",
    "\n",
    "Alayer1 = Activation_layer(tanh,tanh_prime)\n",
    "\n",
    "output_Al1 = Alayer1.forward_prop(output_l1)\n",
    "\n",
    "layer2 = FClayer(4,6)\n",
    "output_l2 = layer2.forward_prop(output_Al1)\n",
    "\n",
    "Alayer2 = Activation_layer(tanh,tanh_prime)\n",
    "output_Al2 = Alayer2.forward_prop(output_l2)\n",
    "\n",
    "layer3 = FClayer(6,2)\n",
    "\n",
    "output_l3 = layer3.forward_prop(output_l2)\n",
    "\n",
    "output_error_dummy = tanh_prime(output_l3) \n",
    "\n",
    "\n",
    "output_error_l3 = layer3.back_prop(output_error_dummy, 0.1)\n",
    "\n",
    "output_error_Al2 = Alayer2.back_prop(output_error_l3)\n",
    "\n",
    "output_error_l2 = layer2.back_prop(output_error_Al2,0.1)\n",
    "\n",
    "# print(layer1.weights)\n",
    "\n",
    "output_error_Al1 = Alayer1.back_prop(output_error_l2)\n",
    "\n",
    "output_error_l1 = layer1.back_prop(output_error_Al1,1)\n",
    "\n",
    "print()\n",
    "# print(layer1.weights)\n",
    "print()\n",
    "\n",
    "output_l1_new =layer1.forward_prop(x_train[0])\n",
    "print(output_l1_new)\n",
    "\n",
    "output_error_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "f = gzip.open('/Users/paarthdudani/Downloads/train-images-idx3-ubyte.gz','r')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 60000\n",
    "\n",
    "f.read(16)\n",
    "\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "\n",
    "data_train = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "data_train = data_train.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "print(np.shape(data_train))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(data_train[0]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test = gzip.open('/Users/paarthdudani/Downloads/t10k-images-idx3-ubyte.gz','r')\n",
    "\n",
    "image_size = 28\n",
    "num_images = 2000\n",
    "\n",
    "buf_test = f_test.read(image_size * image_size * num_images)\n",
    "\n",
    "data_test = np.frombuffer(buf_test, dtype=np.uint8).astype(np.float32)\n",
    "data_test = data_test.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "print(np.shape(data_test))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image_test = np.asarray(data_test[0]).squeeze()\n",
    "plt.imshow(image_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# initializing data to be stored in db\n",
    "Omkar = {'key' : 'Omkar', 'name' : 'Omkar Pathak', \n",
    "'age' : 21, 'pay' : 40000}\n",
    "Jagdish = {'key' : 'Jagdish', 'name' : 'Jagdish Pathak',\n",
    "'age' : 50, 'pay' : 50000}\n",
    "\n",
    "# database\n",
    "db = {}\n",
    "db['Omkar'] = Omkar\n",
    "db['Jagdish'] = Jagdish\n",
    "\n",
    "# For storing\n",
    "# type(b) gives <class 'bytes'>;\n",
    "# b = pickle.dumps(db) \n",
    "# type(b)\n",
    "\n",
    "# loading_pickle = pickle.loads(b)\n",
    "# print(myEntry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open('/Users/paarthdudani/Downloads/cifar-10-python.tar.gz','r')\n",
    "\n",
    "f.read(10*10)\n",
    "\n",
    "\n",
    "image_size = 20\n",
    "num_images = 1000\n",
    "\n",
    "buf = f.read(image_size * image_size * num_images)\n",
    "\n",
    "data_train = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "data_train = data_train.reshape(num_images, image_size, image_size, 1)\n",
    "\n",
    "print(np.shape(data_train))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image = np.asarray(data_train[0]).squeeze()\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN trained on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "MNIST_net = Network()\n",
    "\n",
    "## Pre-processing to convert x_train data into 60000 samples of 1D rows.\n",
    "\n",
    "# class data_set:\n",
    "#     def __init__(self, input):\n",
    "#         self.data = input\n",
    "#         self.stacks = np.shape(self.data)[0]\n",
    "#         self.rows = np.shape(self.data)[1]\n",
    "#         self.cols = np.shape(self.data)[2]\n",
    "#     def NN_friendly_reshaping(self):\n",
    "#         self.data = self.data.reshape(self.stacks,1,self.rows*self.cols)\n",
    "#         return self.data\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = x_train/255\n",
    "x_test = x_test.astype('float32')\n",
    "x_test = x_test/255\n",
    "\n",
    "def NN_friendly_reshaping_func(input):\n",
    "    stacks = np.shape(input)[0]\n",
    "    rows = np.shape(input)[1]\n",
    "    cols = np.shape(input)[2]\n",
    "    return input.reshape(stacks,1,rows*cols)\n",
    "\n",
    "x_train = NN_friendly_reshaping_func(x_train)\n",
    "x_test = NN_friendly_reshaping_func(x_test)\n",
    "\n",
    "\n",
    "## Initializing the network\n",
    "\n",
    "input_size = np.shape(x_train)[-1]\n",
    "\n",
    "def k_to_1_categorizing(input):\n",
    "    \n",
    "    y_train_cat = np.zeros((len(input),10))\n",
    "    for i in range(len(input)):\n",
    "        index = input[i]\n",
    "        y_train_cat[i][index] = 1\n",
    "    return y_train_cat\n",
    "\n",
    "y_train = k_to_1_categorizing(y_train)\n",
    "y_test = k_to_1_categorizing(y_test)\n",
    "\n",
    "MNIST_net.add_layer(FClayer(input_size, 80))\n",
    "MNIST_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "MNIST_net.add_layer(FClayer(80, 50))\n",
    "MNIST_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "MNIST_net.add_layer(FClayer(50, 10))\n",
    "MNIST_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "\n",
    "MNIST_net.use(mse,mse_prime)\n",
    "MNIST_net.train(x_train[0:1000],y_train[0:1000],epochs=40,learning_rate=0.1)\n",
    "\n",
    "out = MNIST_net.predict(x_test[0:3])\n",
    "print('prediction is', out)\n",
    "print()\n",
    "print('Actual output is',y_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN trained on CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "sample_size = 2000\n",
    "\n",
    "def greyscaling_func(input):\n",
    "    stacks = np.shape(input)[0]\n",
    "    rows = np.shape(input)[1]\n",
    "    cols = np.shape(input)[2]\n",
    "    greyscaled_input = np.zeros((stacks,rows,cols),)\n",
    "    for i in range(stacks):\n",
    "        for j in range(rows):\n",
    "            for k in range(cols):\n",
    "                greyscaled_input[i][j][k] = np.mean(input[i][j][k])\n",
    "    \n",
    "    return greyscaled_input.reshape(stacks,1,rows*cols)\n",
    "\n",
    "x_train = greyscaling_func(x_train[0:sample_size])\n",
    "x_test = greyscaling_func(x_test[0:sample_size])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "\n",
    "y_train = k_to_1_categorizing(y_train[0:sample_size])\n",
    "y_test = k_to_1_categorizing(y_test[0:sample_size])\n",
    "\n",
    "input_size = np.shape(x_train)[-1]\n",
    "\n",
    "CIFAR_10_net = Network()\n",
    "CIFAR_10_net.add_layer(FClayer(input_size, 100))\n",
    "CIFAR_10_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "CIFAR_10_net.add_layer(FClayer(100, 50))\n",
    "CIFAR_10_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "CIFAR_10_net.add_layer(FClayer(50, 10))\n",
    "CIFAR_10_net.add_layer(Activation_layer(tanh, tanh_prime))\n",
    "\n",
    "CIFAR_10_net.use(mse,mse_prime)\n",
    "CIFAR_10_net.train(x_train[0:sample_size],y_train[0:sample_size],epochs=35,learning_rate=0.05)\n",
    "\n",
    "out = CIFAR_10_net.predict(x_test[0:3])\n",
    "print('prediction is', out)\n",
    "print()\n",
    "print('Actual output is',y_test[0:3])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
