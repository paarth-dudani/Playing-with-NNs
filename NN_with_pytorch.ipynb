{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/paarthdudani/Downloads/pima-indians-diabetes.csv')\n",
    "\n",
    "dataset = df.to_numpy()\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    " \n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "Y = torch.tensor(Y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "Model = nn.Sequential(\n",
    "    nn.Linear(8,12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12,8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "## Defining the model \n",
    "\n",
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8,12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12,8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward_prop(self,input):\n",
    "        input = self.act1(self.hidden1(input)) \n",
    "        input = self.act2(self.hidden2(input)) \n",
    "        input = self.act_output(self.output(input))\n",
    "        return input\n",
    "\n",
    "Model = PimaClassifier()\n",
    "print(Model)\n",
    "\n",
    "## Defining the loss function for our classifiction problem.\n",
    "\n",
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(Model.parameters(), lr=0.001)\n",
    "\n",
    "np.shape(X[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 0.839552104473114\n",
      "Finished epoch 1, latest loss 0.714958667755127\n",
      "Finished epoch 2, latest loss 0.660731852054596\n",
      "Finished epoch 3, latest loss 0.6279919147491455\n",
      "Finished epoch 4, latest loss 0.6062822937965393\n",
      "Finished epoch 5, latest loss 0.5882455110549927\n",
      "Finished epoch 6, latest loss 0.573871910572052\n",
      "Finished epoch 7, latest loss 0.5533792972564697\n",
      "Finished epoch 8, latest loss 0.5373015403747559\n",
      "Finished epoch 9, latest loss 0.528602123260498\n",
      "Finished epoch 10, latest loss 0.5225642323493958\n",
      "Finished epoch 11, latest loss 0.5160613059997559\n",
      "Finished epoch 12, latest loss 0.511620819568634\n",
      "Finished epoch 13, latest loss 0.5084571242332458\n",
      "Finished epoch 14, latest loss 0.5039725303649902\n",
      "Finished epoch 15, latest loss 0.5115293264389038\n",
      "Finished epoch 16, latest loss 0.5020818114280701\n",
      "Finished epoch 17, latest loss 0.497106671333313\n",
      "Finished epoch 18, latest loss 0.5067750811576843\n",
      "Finished epoch 19, latest loss 0.49897974729537964\n",
      "Finished epoch 20, latest loss 0.4920436441898346\n",
      "Finished epoch 21, latest loss 0.49069973826408386\n",
      "Finished epoch 22, latest loss 0.4887058138847351\n",
      "Finished epoch 23, latest loss 0.4880189299583435\n",
      "Finished epoch 24, latest loss 0.48759397864341736\n",
      "Finished epoch 25, latest loss 0.48735496401786804\n",
      "Finished epoch 26, latest loss 0.484480082988739\n",
      "Finished epoch 27, latest loss 0.4903993010520935\n",
      "Finished epoch 28, latest loss 0.4887310564517975\n",
      "Finished epoch 29, latest loss 0.4879225492477417\n",
      "Finished epoch 30, latest loss 0.48825567960739136\n",
      "Finished epoch 31, latest loss 0.48854896426200867\n",
      "Finished epoch 32, latest loss 0.4890182316303253\n",
      "Finished epoch 33, latest loss 0.4896589517593384\n",
      "Finished epoch 34, latest loss 0.49306753277778625\n",
      "Finished epoch 35, latest loss 0.4949822425842285\n",
      "Finished epoch 36, latest loss 0.49103280901908875\n",
      "Finished epoch 37, latest loss 0.49724847078323364\n",
      "Finished epoch 38, latest loss 0.49190595746040344\n",
      "Finished epoch 39, latest loss 0.4956539273262024\n",
      "Finished epoch 40, latest loss 0.4897628724575043\n",
      "Finished epoch 41, latest loss 0.4901669919490814\n",
      "Finished epoch 42, latest loss 0.4893060028553009\n",
      "Finished epoch 43, latest loss 0.4909844994544983\n",
      "Finished epoch 44, latest loss 0.4893461763858795\n",
      "Finished epoch 45, latest loss 0.49054357409477234\n",
      "Finished epoch 46, latest loss 0.4897082448005676\n",
      "Finished epoch 47, latest loss 0.49023082852363586\n",
      "Finished epoch 48, latest loss 0.4894181489944458\n",
      "Finished epoch 49, latest loss 0.4889834523200989\n",
      "Finished epoch 50, latest loss 0.48664259910583496\n",
      "Finished epoch 51, latest loss 0.48680055141448975\n",
      "Finished epoch 52, latest loss 0.4868342876434326\n",
      "Finished epoch 53, latest loss 0.4859166741371155\n",
      "Finished epoch 54, latest loss 0.48566746711730957\n",
      "Finished epoch 55, latest loss 0.4840144217014313\n",
      "Finished epoch 56, latest loss 0.4848179519176483\n",
      "Finished epoch 57, latest loss 0.47621726989746094\n",
      "Finished epoch 58, latest loss 0.48079177737236023\n",
      "Finished epoch 59, latest loss 0.4823986291885376\n",
      "Finished epoch 60, latest loss 0.4814435541629791\n",
      "Finished epoch 61, latest loss 0.47871002554893494\n",
      "Finished epoch 62, latest loss 0.47691604495048523\n",
      "Finished epoch 63, latest loss 0.47649136185646057\n",
      "Finished epoch 64, latest loss 0.47542884945869446\n",
      "Finished epoch 65, latest loss 0.47543269395828247\n",
      "Finished epoch 66, latest loss 0.47318539023399353\n",
      "Finished epoch 67, latest loss 0.47174111008644104\n",
      "Finished epoch 68, latest loss 0.46896517276763916\n",
      "Finished epoch 69, latest loss 0.4683501720428467\n",
      "Finished epoch 70, latest loss 0.4683235287666321\n",
      "Finished epoch 71, latest loss 0.46819037199020386\n",
      "Finished epoch 72, latest loss 0.4648151397705078\n",
      "Finished epoch 73, latest loss 0.46422189474105835\n",
      "Finished epoch 74, latest loss 0.4635673761367798\n",
      "Finished epoch 75, latest loss 0.46235939860343933\n",
      "Finished epoch 76, latest loss 0.46117112040519714\n",
      "Finished epoch 77, latest loss 0.46092069149017334\n",
      "Finished epoch 78, latest loss 0.4588358998298645\n",
      "Finished epoch 79, latest loss 0.45651400089263916\n",
      "Finished epoch 80, latest loss 0.4551015794277191\n",
      "Finished epoch 81, latest loss 0.45350247621536255\n",
      "Finished epoch 82, latest loss 0.45245206356048584\n",
      "Finished epoch 83, latest loss 0.45076268911361694\n",
      "Finished epoch 84, latest loss 0.4504655599594116\n",
      "Finished epoch 85, latest loss 0.44913965463638306\n",
      "Finished epoch 86, latest loss 0.4482118487358093\n",
      "Finished epoch 87, latest loss 0.44741514325141907\n",
      "Finished epoch 88, latest loss 0.447051465511322\n",
      "Finished epoch 89, latest loss 0.44537344574928284\n",
      "Finished epoch 90, latest loss 0.44362685084342957\n",
      "Finished epoch 91, latest loss 0.44256308674812317\n",
      "Finished epoch 92, latest loss 0.4421837627887726\n",
      "Finished epoch 93, latest loss 0.4407331347465515\n",
      "Finished epoch 94, latest loss 0.4397285580635071\n",
      "Finished epoch 95, latest loss 0.43928858637809753\n",
      "Finished epoch 96, latest loss 0.4376958906650543\n",
      "Finished epoch 97, latest loss 0.43745559453964233\n",
      "Finished epoch 98, latest loss 0.43816065788269043\n",
      "Finished epoch 99, latest loss 0.4374449849128723\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_pred = Model.forward_prop(X_batch)\n",
    "        y_batch = Y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(75.7812)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#     y_pred = model(X)\n",
    "Model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = Model.forward_prop(X)\n",
    "\n",
    "# np.vstack((y_pred,Y))\n",
    "accuracy = 100*(y_pred.round() == Y).float().mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=30, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "Finished epoch 0, latest loss 0.589695155620575\n",
      "Finished epoch 1, latest loss 0.7124692797660828\n",
      "Finished epoch 2, latest loss 0.6820473670959473\n",
      "Finished epoch 3, latest loss 0.6513088941574097\n",
      "Finished epoch 4, latest loss 0.6238135099411011\n",
      "Finished epoch 5, latest loss 0.6101118922233582\n",
      "Finished epoch 6, latest loss 0.5886244177818298\n",
      "Finished epoch 7, latest loss 0.5675727725028992\n",
      "Finished epoch 8, latest loss 0.5523409843444824\n",
      "Finished epoch 9, latest loss 0.5392538905143738\n",
      "Finished epoch 10, latest loss 0.5304855704307556\n",
      "Finished epoch 11, latest loss 0.5236515402793884\n",
      "Finished epoch 12, latest loss 0.5155393481254578\n",
      "Finished epoch 13, latest loss 0.5094025731086731\n",
      "Finished epoch 14, latest loss 0.5054106712341309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(Xbatch)\n\u001b[1;32m     35\u001b[0m ybatch \u001b[38;5;241m=\u001b[39m y[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 36\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mybatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:621\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3172\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3169\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3170\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    " \n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    " \n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    " \n",
    "# define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print(model)\n",
    " \n",
    "# train the model\n",
    "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "n_epochs = 100\n",
    "batch_size = 5\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')\n",
    " \n",
    "# compute accuracy (no_grad is optional)\n",
    "# with torch.no_grad():\n",
    "y_pred = model(X)\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")\n",
    "\n",
    "predictions = (model(X) > 0.5).int()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected 1)\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.600000381469727, 0.20100000500679016, 30.0] => 0 (expected 0)\n",
      "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.24799999594688416, 26.0] => 0 (expected 1)\n",
      "[10.0, 115.0, 0.0, 0.0, 0.0, 35.29999923706055, 0.1340000033378601, 29.0] => 1 (expected 0)\n",
      "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.15800000727176666, 53.0] => 1 (expected 1)\n",
      "[8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.23199999332427979, 54.0] => 0 (expected 1)\n",
      "[4.0, 110.0, 92.0, 0.0, 0.0, 37.599998474121094, 0.19099999964237213, 30.0] => 0 (expected 0)\n",
      "[10.0, 168.0, 74.0, 0.0, 0.0, 38.0, 0.5370000004768372, 34.0] => 1 (expected 1)\n",
      "[10.0, 139.0, 80.0, 0.0, 0.0, 27.100000381469727, 1.440999984741211, 57.0] => 0 (expected 0)\n",
      "[1.0, 189.0, 60.0, 23.0, 846.0, 30.100000381469727, 0.39800000190734863, 59.0] => 1 (expected 1)\n",
      "[5.0, 166.0, 72.0, 19.0, 175.0, 25.799999237060547, 0.5870000123977661, 51.0] => 1 (expected 1)\n",
      "[7.0, 100.0, 0.0, 0.0, 0.0, 30.0, 0.48399999737739563, 32.0] => 1 (expected 1)\n",
      "[0.0, 118.0, 84.0, 47.0, 230.0, 45.79999923706055, 0.5509999990463257, 31.0] => 0 (expected 1)\n",
      "[7.0, 107.0, 74.0, 0.0, 0.0, 29.600000381469727, 0.2540000081062317, 31.0] => 0 (expected 1)\n",
      "[1.0, 103.0, 30.0, 38.0, 83.0, 43.29999923706055, 0.18299999833106995, 33.0] => 1 (expected 0)\n",
      "[1.0, 115.0, 70.0, 30.0, 96.0, 34.599998474121094, 0.5289999842643738, 32.0] => 0 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 1,\n",
    "        num_neurons: int = 5,\n",
    "    ) -> None:\n",
    "        \"\"\"Basic neural network architecture with linear layers\n",
    "        \n",
    "        Args:\n",
    "            num_layers (int, optional): number of hidden layers\n",
    "            num_neurons (int, optional): neurons for each hidden layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(1, num_neurons))\n",
    "\n",
    "        # hidden layers with linear layer and activation\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.Linear(num_neurons, num_neurons), nn.Tanh()])\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(num_neurons, 1))\n",
    "\n",
    "        # build the network\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x.reshape(-1, 1)).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
